name: Scale Testing

on:
  schedule:
    # Run scale tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of scale test to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - load
          - stress
          - endurance
      test_duration:
        description: 'Test duration in seconds'
        required: false
        default: '300'
        type: string
      workers:
        description: 'Number of workers for stress test'
        required: false
        default: '4'
        type: string

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  setup:
    name: Setup Test Environment
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build application
        run: npm run build

      - name: Generate test matrix
        id: matrix
        run: |
          if [ "${{ github.event.inputs.test_type }}" = "all" ] || [ -z "${{ github.event.inputs.test_type }}" ]; then
            echo "matrix=[\"load\", \"stress\", \"volume\"]" >> $GITHUB_OUTPUT
          else
            echo "matrix=[\"${{ github.event.inputs.test_type }}\"]" >> $GITHUB_OUTPUT
          fi

  scale-tests:
    name: Scale Tests
    runs-on: ubuntu-latest
    needs: setup
    strategy:
      fail-fast: false
      matrix:
        test-type: ${{ fromJson(needs.setup.outputs.test-matrix) }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build application
        run: npm run build

      - name: Setup test database
        run: |
          mkdir -p data
          npm run db:migrate
          npm run db:seed

      - name: Start server in background
        run: |
          npm run start:production &
          echo $! > server.pid
          sleep 10  # Wait for server to start

      - name: Wait for server to be ready
        run: |
          timeout 30 bash -c 'until curl -s http://localhost:3000/api/v1/health; do echo "Waiting for server..."; sleep 2; done'

      - name: Install Artillery (for load tests)
        if: matrix.test-type == 'load'
        run: npm install -g artillery@latest

      - name: Run Load Tests
        if: matrix.test-type == 'load'
        run: |
          echo "ðŸš€ Starting Artillery load test..."
          artillery run artillery-config.yml --output artillery-report.json
          
          # Generate HTML report
          artillery report artillery-report.json --output artillery-report.html
        continue-on-error: true

      - name: Run Stress Tests
        if: matrix.test-type == 'stress'
        run: |
          echo "ðŸš€ Starting custom stress test..."
          DURATION=${{ github.event.inputs.test_duration || '300' }}
          WORKERS=${{ github.event.inputs.workers || '4' }}
          
          node scripts/stress-test-runner.js \
            --duration $(($DURATION * 1000)) \
            --workers $WORKERS \
            --requestsPerWorker 1000 \
            --outputDir ./stress-test-results
        continue-on-error: true

      - name: Run Volume Tests
        if: matrix.test-type == 'volume'
        run: |
          echo "ðŸš€ Starting volume tests..."
          npm run test:performance:verbose
        continue-on-error: true

      - name: Run Performance Monitoring
        run: |
          echo "ðŸ“Š Starting performance monitoring..."
          timeout 30 node scripts/performance-monitor.js \
            --interval 5000 \
            --outputDir ./performance-data || true

      - name: Collect system metrics
        run: |
          echo "ðŸ“ˆ Collecting system metrics..."
          
          # CPU and memory info
          echo "=== System Information ===" > system-metrics.txt
          echo "CPU: $(nproc) cores" >> system-metrics.txt
          echo "Memory: $(free -h | awk '/^Mem:/ {print $2}')" >> system-metrics.txt
          echo "Disk: $(df -h / | awk 'NR==2 {print $4}')" >> system-metrics.txt
          echo "" >> system-metrics.txt
          
          # Process information
          echo "=== Process Information ===" >> system-metrics.txt
          ps aux | grep -E "(node|npm)" | grep -v grep >> system-metrics.txt || true
          echo "" >> system-metrics.txt
          
          # Network information
          echo "=== Network Information ===" >> system-metrics.txt
          netstat -tulpn | grep :3000 >> system-metrics.txt || true

      - name: Stop server
        if: always()
        run: |
          if [ -f server.pid ]; then
            kill $(cat server.pid) || true
            rm server.pid
          fi
          pkill -f "node.*server" || true

      - name: Analyze results
        run: |
          echo "ðŸ“Š Analyzing test results..."
          
          # Create results summary
          echo "# Scale Test Results - ${{ matrix.test-type }}" > test-summary.md
          echo "**Date:** $(date)" >> test-summary.md
          echo "**Test Type:** ${{ matrix.test-type }}" >> test-summary.md
          echo "**Duration:** ${{ github.event.inputs.test_duration || '300' }}s" >> test-summary.md
          echo "" >> test-summary.md
          
          # Add load test results
          if [ -f artillery-report.json ] && [ "${{ matrix.test-type }}" = "load" ]; then
            echo "## Load Test Results" >> test-summary.md
            
            # Extract key metrics from Artillery report
            if command -v jq >/dev/null 2>&1; then
              TOTAL_REQUESTS=$(jq -r '.aggregate.counters["http.requests"] // 0' artillery-report.json)
              SUCCESS_RATE=$(jq -r '.aggregate.counters["http.codes.200"] // 0' artillery-report.json)
              AVG_RESPONSE=$(jq -r '.aggregate.latency.mean // 0' artillery-report.json)
              P95_RESPONSE=$(jq -r '.aggregate.latency.p95 // 0' artillery-report.json)
              
              echo "- **Total Requests:** $TOTAL_REQUESTS" >> test-summary.md
              echo "- **Successful Requests:** $SUCCESS_RATE" >> test-summary.md
              echo "- **Average Response Time:** ${AVG_RESPONSE}ms" >> test-summary.md
              echo "- **95th Percentile:** ${P95_RESPONSE}ms" >> test-summary.md
            fi
            echo "" >> test-summary.md
          fi
          
          # Add stress test results
          if [ -d stress-test-results ] && [ "${{ matrix.test-type }}" = "stress" ]; then
            echo "## Stress Test Results" >> test-summary.md
            
            # Find the latest results file
            LATEST_RESULT=$(ls -t stress-test-results/stress-test-*.json 2>/dev/null | head -1)
            if [ ! -z "$LATEST_RESULT" ] && [ -f "$LATEST_RESULT" ]; then
              if command -v jq >/dev/null 2>&1; then
                TOTAL_REQ=$(jq -r '.results.totalRequests // 0' "$LATEST_RESULT")
                SUCCESS_REQ=$(jq -r '.results.successfulRequests // 0' "$LATEST_RESULT")
                REQ_PER_SEC=$(jq -r '.results.requestsPerSecond // 0' "$LATEST_RESULT")
                AVG_RESP=$(jq -r '.results.averageResponseTime // "0ms"' "$LATEST_RESULT")
                ERROR_RATE=$(jq -r '.results.errorRate // "0%"' "$LATEST_RESULT")
                
                echo "- **Total Requests:** $TOTAL_REQ" >> test-summary.md
                echo "- **Successful Requests:** $SUCCESS_REQ" >> test-summary.md
                echo "- **Requests per Second:** $REQ_PER_SEC" >> test-summary.md
                echo "- **Average Response Time:** $AVG_RESP" >> test-summary.md
                echo "- **Error Rate:** $ERROR_RATE" >> test-summary.md
              fi
            fi
            echo "" >> test-summary.md
          fi
          
          # Add system metrics
          if [ -f system-metrics.txt ]; then
            echo "## System Metrics" >> test-summary.md
            echo '```' >> test-summary.md
            cat system-metrics.txt >> test-summary.md
            echo '```' >> test-summary.md
          fi

      - name: Upload test artifacts
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: scale-test-results-${{ matrix.test-type }}
          path: |
            artillery-report.*
            stress-test-results/
            performance-data/
            test-summary.md
            system-metrics.txt
          retention-days: 30

      - name: Check test thresholds
        id: threshold-check
        run: |
          echo "ðŸ” Checking performance thresholds..."
          
          FAILED=false
          ISSUES=""
          
          # Check Artillery results (Load Tests)
          if [ -f artillery-report.json ] && [ "${{ matrix.test-type }}" = "load" ]; then
            if command -v jq >/dev/null 2>&1; then
              ERROR_RATE=$(jq -r '.aggregate.rates.error // 0' artillery-report.json)
              P95_LATENCY=$(jq -r '.aggregate.latency.p95 // 0' artillery-report.json)
              
              # Error rate should be < 5%
              if (( $(echo "$ERROR_RATE > 0.05" | bc -l) )); then
                FAILED=true
                ISSUES="$ISSUES\n- Error rate too high: ${ERROR_RATE}%"
              fi
              
              # P95 latency should be < 1000ms
              if (( $(echo "$P95_LATENCY > 1000" | bc -l) )); then
                FAILED=true
                ISSUES="$ISSUES\n- P95 latency too high: ${P95_LATENCY}ms"
              fi
            fi
          fi
          
          # Check stress test results
          if [ -d stress-test-results ] && [ "${{ matrix.test-type }}" = "stress" ]; then
            LATEST_RESULT=$(ls -t stress-test-results/stress-test-*.json 2>/dev/null | head -1)
            if [ ! -z "$LATEST_RESULT" ] && [ -f "$LATEST_RESULT" ]; then
              if command -v jq >/dev/null 2>&1; then
                ERROR_RATE=$(jq -r '.results.errorRate // "0%"' "$LATEST_RESULT" | sed 's/%//')
                REQ_PER_SEC=$(jq -r '.results.requestsPerSecond // 0' "$LATEST_RESULT")
                
                # Error rate should be < 5%
                if (( $(echo "$ERROR_RATE > 5" | bc -l) )); then
                  FAILED=true
                  ISSUES="$ISSUES\n- Error rate too high: ${ERROR_RATE}%"
                fi
                
                # Should handle at least 100 requests/second
                if (( $(echo "$REQ_PER_SEC < 100" | bc -l) )); then
                  FAILED=true
                  ISSUES="$ISSUES\n- Throughput too low: ${REQ_PER_SEC} req/sec"
                fi
              fi
            fi
          fi
          
          echo "failed=$FAILED" >> $GITHUB_OUTPUT
          echo -e "issues=$ISSUES" >> $GITHUB_OUTPUT

      - name: Comment on PR (if applicable)
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let comment = '## ðŸ§ª Scale Test Results - ${{ matrix.test-type }}\n\n';
            
            if (fs.existsSync('test-summary.md')) {
              const summary = fs.readFileSync('test-summary.md', 'utf8');
              comment += summary;
            }
            
            const failed = '${{ steps.threshold-check.outputs.failed }}' === 'true';
            const issues = '${{ steps.threshold-check.outputs.issues }}';
            
            if (failed) {
              comment += '\n\n## âŒ Performance Issues Detected\n';
              comment += issues;
              comment += '\n\n**Action Required:** Please address these performance issues before merging.';
            } else {
              comment += '\n\n## âœ… All Performance Thresholds Met';
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: scale-tests
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v5
        with:
          path: ./test-results

      - name: Install analysis tools
        run: |
          sudo apt-get update
          sudo apt-get install -y jq bc

      - name: Generate comprehensive report
        run: |
          echo "ðŸ“Š Generating comprehensive performance report..."
          
          cat > performance-report.md << 'EOF'
          # Comprehensive Scale Test Report
          
          **Date:** $(date)
          **Commit:** ${{ github.sha }}
          **Branch:** ${{ github.ref_name }}
          **Triggered by:** ${{ github.event_name }}
          
          ## Test Summary
          
          EOF
          
          # Process each test type result
          for result_dir in ./test-results/scale-test-results-*; do
            if [ -d "$result_dir" ]; then
              test_type=$(basename "$result_dir" | sed 's/scale-test-results-//')
              echo "Processing $test_type results..."
              
              echo "### $test_type Test Results" >> performance-report.md
              
              if [ -f "$result_dir/test-summary.md" ]; then
                cat "$result_dir/test-summary.md" >> performance-report.md
              fi
              
              echo "" >> performance-report.md
            fi
          done
          
          # Add recommendations
          cat >> performance-report.md << 'EOF'
          
          ## Recommendations
          
          Based on the test results, consider the following optimizations:
          
          1. **Database Optimization**: Review slow queries and add appropriate indexes
          2. **Memory Management**: Monitor for memory leaks in long-running processes
          3. **Connection Pooling**: Optimize database connection pool settings
          4. **Caching Strategy**: Implement caching for frequently accessed data
          5. **Load Balancing**: Consider horizontal scaling for high-traffic scenarios
          
          ## Next Steps
          
          - [ ] Review performance metrics against baseline
          - [ ] Address any performance regressions
          - [ ] Update performance benchmarks if improvements were made
          - [ ] Consider infrastructure scaling if growth is expected
          
          EOF

      - name: Upload comprehensive report
        uses: actions/upload-artifact@v3
        with:
          name: comprehensive-performance-report
          path: performance-report.md
          retention-days: 30

      - name: Create issue for performance regressions
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let issueBody = '## Performance Regression Detected\n\n';
            issueBody += 'The scale testing workflow has detected performance issues that require attention.\n\n';
            issueBody += '### Test Details\n';
            issueBody += `- **Commit:** ${context.sha}\n`;
            issueBody += `- **Branch:** ${context.ref}\n`;
            issueBody += `- **Workflow Run:** ${context.runId}\n\n`;
            
            if (fs.existsSync('performance-report.md')) {
              const report = fs.readFileSync('performance-report.md', 'utf8');
              issueBody += '### Performance Report\n';
              issueBody += report;
            }
            
            issueBody += '\n\n### Action Required\n';
            issueBody += 'Please investigate and address the performance issues identified in this report.\n';
            issueBody += 'Consider reviewing recent changes that may have impacted performance.';
            
            const { data: issues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: 'performance,regression',
              state: 'open'
            });
            
            // Only create issue if one doesn't already exist
            if (issues.length === 0) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: 'Performance Regression Detected in Scale Tests',
                body: issueBody,
                labels: ['performance', 'regression', 'high-priority']
              });
            }