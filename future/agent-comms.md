Asynchronous Multi‑Agent LLM Collaboration – Product Requirements Document (PRD)

This PRD outlines a system for asynchronous multi-agent collaboration using Large Language Models (LLMs) to improve software development workflows (coding, testing, refactoring, documentation). The solution builds on a Task Kanban framework (like task-kanban-mcp) backed by a vector database (ChromaDB) for memory. We define three key subsystems – (1) a Critic Role Agent for automated review and feedback, (2) an Agent Messaging System for structured asynchronous communication, and (3) a Cross-Agent Pattern Sharing mechanism for global learning and knowledge reuse. Each section below details the features, architecture, data models, and example workflows for these components, with references to state-of-the-art research (2023–2025) and real-world implementations. Finally, we propose performance metrics to evaluate success (e.g. critique effectiveness, knowledge reuse rate, consensus correctness).

1. Critic Role Agent System

1.1 Features and Objectives

The Critic Role Agent is an LLM-based agent dedicated to reviewing and critiquing the work produced by other agents. Its goal is to iteratively improve outputs by providing feedback on code quality, test logic, documentation clarity, and more. This agent embodies the Evaluator-Optimizer pattern – one agent generates a solution, and the Critic agent evaluates it and suggests improvements, looping until quality standards are met ￼. Key features include:
	•	Automated Code Review & Testing Feedback: The Critic can analyze source code for bugs, style issues, or inefficiencies and propose corrections. It similarly reviews test cases (coverage, correctness) and documentation (accuracy, completeness).
	•	Self-Refinement Loops: The Critic enables self-refine iterations where an agent’s initial output is refined through feedback. This implements research like Self-Refine (2023), which showed that iterative self-feedback can yield higher-quality results without additional training ￼ ￼.
	•	Debate-Style Prompting: The system supports a mode where multiple agents (or multiple perspectives of a single agent) debate a solution. The Critic can orchestrate a “debate” between solutions – e.g. two code approaches – highlighting pros/cons until a consensus emerges ￼. This draws on multi-agent debate (MAD) research, where agents discuss and refine answers in rounds, improving factuality and reasoning through critique of each other’s answers ￼.
	•	Reflection-on-Reflection: Beyond one-shot feedback, the Critic can engage in multi-level reflection. For example, it may critique an initial solution, then reflect on its own feedback in a subsequent step to catch any oversight or to refine the feedback (a “critic of the critic”). This recursive reflection draws on ideas from Reflexion (Shinn et al. 2023), where an agent uses verbal self-evaluation and stores it to improve subsequent attempts ￼. By stacking reflection loops, the system mitigates shallow critiques and encourages deeper analysis.
	•	Asynchronous Operation: Crucially, the Critic agent operates asynchronously. It does not block the main workflow; instead, it monitors for new outputs to evaluate (e.g. a “code complete” event) via the messaging system (detailed in Section 2). The Critic’s feedback is posted as messages that the original agent (or a different fixer agent) can consume whenever ready. This decoupling allows, for instance, a coding agent to continue with other tasks or multiple Critic agents to review in parallel. Techniques like a message queue (producer/consumer) enable this async pattern ￼. The Critic may also bundle multiple pending feedback items into one batch to optimize LLM context usage ￼ ￼.

Overall, the Critic Role Agent aims to introduce an automated “quality gate” in the multi-agent pipeline, increasing correctness and robustness of outputs through iterative refinement and independent verification.

1.2 Architecture & Components

The Critic agent can be thought of as a specialized service in the multi-agent architecture. In a typical deployment, when an agent (e.g. a coding agent) produces an output (code, test, etc.), that output along with context is sent as a message to the Critic’s inbox or placed in a shared message pool (see Messaging System). The Critic architecture includes:
	•	Input Handler: to retrieve the item to critique (code snippet, test results, documentation draft) from the message system. It may fetch additional context such as the original requirements or the agent’s reasoning leading to the output, to better understand intent.
	•	Critique Prompt Generator: which formulates a prompt for the LLM-based critic. This typically includes the output to review, guidelines or criteria (coding standards, correctness conditions), and possibly an instruction like “Act as a code reviewer and point out any issues and improvements.” If debate mode is enabled, this component may generate a prompt that includes multiple agent outputs side-by-side and asks the LLM to compare them or have them “discuss.”
	•	LLM Critic Model: the core large language model (e.g. GPT-4 or another) that generates the critique. The model output could be a list of identified problems with explanations and recommended fixes. Research by Guo et al. (2024) suggests an effective pattern: an LLM-based Critic analyzes the full trajectory of agent decisions (e.g. conversation and actions in an episode) and produces an evaluation summary ￼. In our context, the Critic similarly will analyze the sequence of steps leading to the output (if available) to pinpoint where things might have gone wrong.
	•	Feedback Formatter: to structure the critique into a message format understood by other agents. For example, it might label the critique with an intent: "critique" field, reference the relevant taskId or output identifier, and include a brief summary (e.g. “Found 3 bugs and style issues”) along with the detailed feedback. This message is then posted back to the communication medium (message pool or directly to the producer agent’s queue).

The Critic can also function in a coordinator capacity for debate or self-reflection loops. In advanced scenarios, we can instantiate two LLM agents – one as Critic and another as a Coordinator. The Critic evaluates performance, and the Coordinator uses that evaluation to suggest structural changes ￼ ￼. For example, in organized multi-agent teams, Guo et al. used a Critic-Coordinator duo: the Critic scores agents’ behaviors, and the Coordinator reflects on those scores plus external metrics to propose changes in strategy ￼ ￼. We can adapt this idea: the Critic flags issues, and a high-level planner agent could then adjust the workflow or reassign tasks (akin to a coordinator role). However, initially the focus is on the Critic’s feedback loop to the original task agent.

Asynchronous flow: The Critic agent does not need to be invoked synchronously in the middle of the original agent’s process. Instead, upon task completion or at defined checkpoints, the output is sent to the Critic. The Critic processes it in the background. When ready, it posts a critique message. The original agent (or another designated agent, e.g. a “Fixer” role) will at some point check for critique messages. This is facilitated by the messaging subsystem – e.g. the coding agent subscribes to critique messages about its task. This decoupling allows multiple Critic agents to scale (for different tasks or different aspects like security review vs. style review), and the producing agents can continue other work and incorporate feedback when it arrives ￼.

1.3 Data Model

The Critic agent’s primary data inputs and outputs are messages (further described in Section 2). Key data elements for a Critique message include:
	•	taskId – links the feedback to the specific task or artifact (so that the right agent or process can associate it with the code/test it wrote).
	•	sender – the Critic agent’s identifier/role.
	•	intent – e.g. "critique" or more specifically "code_review", "test_feedback", "doc_review" depending on the content.
	•	summary – a short summary of the critique (one or two sentences). For example: “Code review: 3 potential null-pointer bugs found and documentation unclear on function XYZ.”
	•	details – the full critique text generated by the LLM. This can be structured (e.g. a list of issues each with description and suggested fix).
	•	severity or confidence (optional) – e.g. a rating of how important the issues are, or how confident the Critic is. This could help the receiving agent prioritize fixes (first fix critical errors, then style improvements).
	•	embedding – an embedding vector of the critique content (or perhaps of the summary) for semantic retrieval (useful if another agent later wants to search the knowledge base for similar issues or feedback).

These messages are stored in the vector database (ChromaDB) as part of the global memory, enabling future queries. For example, if a similar code pattern appears later, an agent could search the memory and find that a Critic previously flagged a bug in that pattern, thus reusing that insight.

Internally, the Critic agent may also maintain a prompt template (for generating its LLM calls) and some criteria data (e.g. style guidelines or acceptance criteria) which could be stored in a knowledge base. For instance, a global critique knowledge base might include common pitfalls or a checklist (like “SQL injection risk”, “off-by-one error”) that the Critic references when reviewing code. This can be updated over time as the Critic learns new failure modes (potential integration with Cross-Agent Pattern Sharing in Section 3).

1.4 Example Workflow

Use Case: A coding agent (let’s call it the “Engineer” agent) has written a new function and completed its task. Now, the Critic agent should review this code and provide feedback.
	•	Step 1: Developer Agent publishes output: The Engineer agent pushes a message to the shared log, for example: intent: "output.complete", taskId: 42, content: "<code snippet>", role: "Engineer", summary: "Completed code for feature X". This signals that code for task #42 is ready for review. This message’s metadata could indicate it expects a critique.
	•	Step 2: Critic Agent retrieves task: The Critic agent, which subscribes to either all “output.complete” events or specifically those from Engineers, picks up this message asynchronously. (The system might alternatively route it to Critic explicitly by send_to: Critic field.)
	•	Step 3: Critique Generation: The Critic’s Input Handler loads the code and related context (it may fetch the problem description or previous conversation from memory via the taskId). The Critique Prompt Generator prepares a prompt: e.g. “You are an expert code reviewer. Below is code for Feature X written by another agent. Review it for any bugs, inefficiencies, or style issues. Provide feedback with suggestions for improvement.” The code is attached, possibly along with the original requirement for context. The LLM Critic model processes this and outputs a critique.
	•	Step 4: Posting Critique Message: The Critic agent formats this into a message: intent: "critique", taskId: 42, sender: "Critic", summary: "Code review found potential null-pointer dereference and suggests refactoring loop logic.", and detailed content listing the issues. This message is published to the Shared Message Pool (or to the Engineer’s message queue).
Example: Criticize-Reflect architecture from research ￼. In this diagram (from Guo et al. 2024), an LLM Critic evaluates agent trajectories after each episode, and an LLM Coordinator uses that feedback to adjust the team’s organizational prompt ￼ ￼. Our system’s Critic agent plays a similar evaluator role: after each task episode, it critiques outputs and (optionally) a coordinator or the original agent uses that feedback to refine future actions.
	•	Step 5: Consuming Feedback: The Engineer agent periodically queries for new messages related to its tasks. It finds the critique. The Engineer then enters a refinement loop: it updates the code according to the feedback. For instance, fix the null-pointer issue and adjust the loop logic. It might then mark task #42 as “refined” and even send the updated code back to the Critic for confirmation. This loop can repeat – effectively an asynchronous Self-Refine loop, where the agent and Critic ping-pong improvements until the Critic has no further complaints or a quality threshold is reached ￼. Each iteration is asynchronous (the Engineer could do other work while waiting for the Critic’s second pass, etc.).
	•	Optional Debate Scenario: Suppose two different agents implemented two competing solutions for the same problem (or one agent made two attempts). We can spawn a debate using the Critic. The Critic could generate a comparative evaluation: “Solution A vs Solution B.” Alternatively, two Critic agents could each champion one solution and a third agent (referee) decides which is better. This debate process would occur in rounds: each agent sees the other’s solution and perhaps an initial critique, then tries to argue or improve on it, and so on ￼. The final outcome is a consensus solution that hopefully is more correct (this approach is inspired by ChatEval and other multi-LLM evaluator frameworks that achieved closer-to-human evaluation by multi-agent discussion ￼ ￼). While debate-style prompting is powerful, it is used when high-stakes correctness is needed (e.g. security-critical code) due to higher computation cost (multiple agents & rounds).

Asynchronous Timing: In practice, the Critic agent could be running on a separate process or thread. The messaging system ensures the Engineer’s output triggers a critique job, but the Engineer doesn’t idle – perhaps it picks up a new task or performs self-testing in parallel. Once the Critic’s message arrives, the Engineer merges those suggestions. This design prevents pipeline stalls and allows parallel critiques on multiple outputs. If numerous tasks finish simultaneously, multiple Critic agents (or a single agent handling them queued) can operate concurrently.

Through this Critic subsystem, the platform continuously self-monitors and improves its outputs. It resembles having an automated “QA Engineer” in the loop. Indeed, frameworks like MetaGPT explicitly include a QA agent that generates tests and reviews the engineer’s code ￼. Our Critic generalizes that idea to all forms of critique (code review, test review, documentation proofreading, etc.) and operates asynchronously for efficiency.

2. Agent Messaging System

2.1 Features and Requirements

To enable effective collaboration, agents need a robust communication mechanism. The Agent Messaging System provides an asynchronous, structured message-passing interface for all LLM agents. Key requirements and features include:
	•	Publish/Subscribe Message Pool: Agents do not directly call each other; instead, they post messages to a shared medium and retrieve relevant messages ￼. This decouples senders and receivers and allows messages to persist for later retrieval. It also supports one-to-many broadcasts if needed (e.g. an announcement to all agents).
	•	Asynchronous Delivery via Queues: The system should handle asynchronous communication reliably. Each agent can have an incoming message queue where messages accumulate until the agent processes them ￼. A producer/consumer model is used: agents producing messages are “publishers,” and receiving agents (or any interested agent) are “subscribers.” This way, an agent can be idle (no CPU use) when no messages are for it, and wake up to handle a batch of messages, then go idle again ￼. This design prevents needless polling and scales to many agents.
	•	Structured Message Format: Each message has defined fields to make filtering and routing efficient. As suggested, fields include at least:
	•	intent – the purpose or type of message (e.g. "task_assignment", "output.complete", "critique", "request.help", "status.update"). This is used to route and prioritize messages. For instance, an agent might ignore messages not intended for it or not of a type it handles.
	•	taskId – an identifier linking the message to a particular task or context (if applicable). This is crucial for grouping messages in the Kanban/task system. All messages related to Task #42, for example, can be fetched together.
	•	sender – who sent the message (could be agent ID or role name, e.g. “ProductManagerAgent” or “CriticAgent”).
	•	recipient – optional explicit target. Often, instead of a direct address, the system uses intent and task to determine the audience. But if a message is specifically meant for one agent, this can be set (e.g. “send_to: EngineerAgent1”).
	•	summary – a brief summary or title of the message. This helps other agents scan the message log without loading full content. It is also useful for memory (embedding this summary for quick semantic search).
	•	content – the full content/payload of the message (which could be a natural language instruction, a piece of code, a file diff, a question, etc., depending on intent).
	•	timestamp or chronological order info.
	•	embedding – a vector embedding of the content or summary for similarity search (populating the ChromaDB). Storing embeddings allows agents to query past messages semantically (for example, “find any messages about sorting algorithms in past tasks”).
	•	Other metadata: e.g. priority flags, status (new, read, acknowledged), etc., and possibly references to related message IDs (if replying or a part of a thread).
	•	Filtering and Retrieval: Agents should be able to query the message store flexibly. Options include:
	•	Retrieve by taskId (get all messages for my current task).
	•	Retrieve by intent (e.g. get all “critique” type messages addressed to me).
	•	Time-based queries (since last check).
	•	Semantic queries using embeddings (e.g. find any message similar to X).
	•	Subscription filters: an agent can register interest in certain intents or tasks so that the system automatically pushes those messages to its queue or raises an event.
The system may implement channels or topics for common intents. For example, a “Task42” channel for task-specific comms, and a “GlobalAnnouncements” channel for broadcast. Agents can subscribe/unsubscribe dynamically.
	•	Persistence: All messages are persisted in the ChromaDB (or a paired relational store for metadata) as a log. This ensures no information is lost and enables historical analysis (for the Cross-Agent learning in Section 3). The vector DB stores the embedding + key fields, and the raw content could be stored in a traditional database or object store (with the DB storing a pointer). ChromaDB in particular can store documents (content) with metadata, which fits our use case well: we store each message content (document) with metadata fields (taskId, sender, intent, etc.) and an embedding.
	•	Efficiency Considerations: As agents might produce many messages, we need to avoid overload. Techniques include:
	•	Limiting redundant messages (for example, instead of broadcasting to 10 agents separately, one broadcast entry can be posted and 10 agents read it).
	•	Summarizing long threads: if a task has 100 messages, an agent joining later might not ingest all. We could maintain a running summary in the summary field or have a “summary message” every N updates.
	•	Sparse communication topologies: Recent research suggests not every agent needs to talk to every other for effective collaboration (sparse topologies can reduce cost while retaining performance) ￼ ￼. We might configure communication networks (e.g. only specific pairs communicate directly, others via an orchestrator) depending on roles to limit chatter.

2.2 Architecture & Data Model

The messaging system can be visualized in two parts: a global message store (the shared pool) and agent-side interfaces (APIs or clients through which agents publish/receive).

Global Message Pool: Conceptually a central hub where all messages live. MetaGPT’s design directly inspires this: “MetaGPT uses a global message pool to store information that allows agents to exchange messages… Agents publish their structured messages in the pool and can access messages from other agents transparently, removing the need to ask others directly ￼.” In our system, the pool is implemented via ChromaDB plus auxiliary structures for queues:
	•	We create, for example, a Chroma collection for messages. Each message is an entry with its embedding and metadata. We might also maintain separate collections per message type or per task if needed (to aid filtering), but a single collection with metadata filters is usually sufficient given Chroma’s support for metadata filtering.
	•	Additionally, for real-time delivery, each agent could have an in-memory queue. A background process or trigger can monitor the DB for new messages matching an agent’s subscriptions and push them to that agent’s queue (or notify the agent). Alternatively, agents periodically poll for new messages since last check (this is simpler but might introduce slight latency; given tasks are not usually millisecond-urgent, a polling interval of a few seconds is acceptable).

Agent Interface: Each agent is provided with a messaging API:
	•	publish_message(message) – to format and send a message to the pool.
	•	get_new_messages(filter) – to retrieve messages intended for that agent (or a specific task). The filter can default to “recipient == me OR (recipient is null and taskId in my current tasks or intent in my subscriptions)”.
	•	Possibly convenience methods like await_message(intent, taskId, timeout) if an agent needs to wait for a specific response (though busy-wait is discouraged; better to use async eventing).

The message data model follows the structured format discussed. For example, an entry in JSON form might look like:

{
  "id": "uuid-1234",
  "taskId": 42,
  "intent": "critique",
  "sender": "CriticAgent",
  "recipient": "EngineerAgent",
  "summary": "Code critique for Task 42: potential bug in data parsing",
  "content": "Line 10: possible null pointer dereference...\nLine 24: inefficient loop...\nRecommendation: ...",
  "embedding": [0.123, 0.456, ... 768-dim vector ...],
  "timestamp": "2025-07-27T11:45:00Z"
}

This would be stored in the vector DB with metadata = {“taskId”:42, “intent”:“critique”, “sender”:“CriticAgent”, …} and the embedding vector. When the EngineerAgent queries with taskId:42 AND intent:critique filters, it can retrieve this. If it uses a semantic search (embedding-based), it could query by embedding of “bug” to find all critiques about bugs, etc.

Asynchronous Mechanism: In MegaAgent (Wu et al. 2024), a similar producer-consumer queue mechanism is described: each agent has a message queue and cycles through states – Idle (no message), Processing (process a batch of messages), Responding (output results) ￼ ￼. We adopt this model:
	•	When an agent is free, it checks its queue. If messages present, it may take one or a batch and handle them.
	•	While it’s processing, any new incoming messages queue up and will be handled in the next cycle (ensuring atomic processing of one batch at a time).
	•	After processing, the agent may produce response messages. Then it goes back to Idle or picks up the next message.

This guarantees asynchronous yet ordered handling per agent. It also enables batch processing: an agent can group multiple small messages and process them in one LLM call to improve throughput ￼ (e.g. reading 5 related instructions together).

Illustration of the agent communication protocol and memory (inspired by MetaGPT and Microsoft Autogen) ￼ ￼. Left: Agents communicate via a Shared Message Pool using structured messages. Each message has fields like sender, recipient, content, and cause/intent (e.g. an Architect agent publishes design info which the Engineer agent subscribes to). This decoupled publish/subscribe model means agents don’t directly call each other, improving scalability ￼. Right: Each agent (example: Engineer “Alex”) uses iterative programming with feedback. After writing code, the agent executes tests; on errors, it retrieves relevant past messages (PRD, design, code history) from memory, and debugs accordingly. This shows how the messaging system combined with a memory (history messages and vector retrieval) enables cross-referencing past knowledge during task execution.

(Figure 2 above is a representative workflow: the Project Manager issues a “WriteTasks” instruction to the Engineer via the message pool; the Engineer subscribes to those tasks, writes code, then uses feedback from execution and stored knowledge to improve the code. All interactions are through structured messages in the pool, enabling asynchronous teamwork.)

2.3 Example Workflows

Workflow A: Task Assignment and Updates – The Product Manager agent creates a new task (say “Implement feature X”) and publishes a message:
	•	intent: "task_assignment", taskId: 100, recipient: EngineerTeam (could be a group or broadcast), content: "Feature X needs to be implemented with requirements A, B, C...".
	•	All Engineer agents subscribed to new tasks see this. Suppose EngineerAgent1 takes it. It might reply with intent: "task_claim", taskId: 100, sender: Engineer1, content: "I will work on Feature X" – so everyone knows it’s taken (this could also update a Kanban status).
	•	EngineerAgent1 during work might send intent: "status_update", taskId: 100, content: "50% done, stuck on API issue" which the Product Manager or others can read. If it needs help, it could send intent: "request_help", taskId:100, content: "Looking for best way to optimize algorithm Y", and any agent (maybe a specialized “ResearchAgent”) could respond with advice.
	•	Once done, EngineerAgent1 sends intent: "output.complete", taskId:100, with the code attached (or a link if large).

Workflow B: QA Feedback Loop – Building on the above, once the engineer marks output complete, a QA agent (or the Critic agent playing QA role) will get that message. It generates a test or critique:
	•	QA agent publishes intent: "test_results", taskId:100, sender: QA, content: "Test failed on edge case Z."
	•	The message system routes this to EngineerAgent1 (by taskId and perhaps recipient: Engineer1). The engineer receives it in its queue, fixes the issue, and replies intent: "output.updated", taskId:100, content: "Fixed edge case Z, please re-test."
	•	QA agent picks it up, retests, and finally sends intent: "approval", taskId:100, content: "All tests passed. Feature X completed."

Throughout these exchanges, all messages are logged in the pool with the structured schema. Agents not directly involved could still query or observe if needed (e.g. a Project Manager may query all status updates across tasks to create a progress report).

Workflow C: Multi-Agent Discussion (Consensus Building) – Sometimes agents need to reach a consensus (for example, two modules built by different engineers need to agree on an interface). The messaging system can facilitate a mini-debate:
	•	EngineerA proposes an interface via a intent: "proposal" message.
	•	EngineerB responds with a intent: "proposal.feedback" stating concerns or alternative suggestions.
	•	A loop of back-and-forth message exchange ensues, mediated by the shared pool. They might label threads by adding e.g. threadId in metadata for that specific discussion.
	•	If needed, a mediator agent (or Critic as referee) could join. For instance, Critic sees prolonged debate and posts intent: "consensus_recommendation", summarizing both sides and suggesting a compromise.
	•	Finally, one engineer posts intent: "consensus_reached" with the agreed decision.

Because all these are asynchronous messages, EngineerA and EngineerB don’t have to be waiting at the exact same moment; one can post, and hours later the other can reply (though in practice these are fast to keep development moving). The log of their dialogue is stored (and could be later mined for how disagreements are resolved, feeding into pattern learning).

In summary, the Agent Messaging System provides the nervous system of the multi-agent setup – a way to exchange information reliably, asynchronously, and in a structured manner. It draws from proven designs in multi-agent frameworks: for example, MetaGPT’s publish-subscribe message pool for collaborative coding ￼ and Microsoft’s Autogen which treats inter-agent communication as “conversations” to be programmed and flexibly routed ￼ ￼. By combining a global message bus with agent-specific filters/queues, the system ensures that collaboration can scale to many agents and tasks without confusion.

3. Cross-Agent Pattern Sharing (Global Memory & Learning)

3.1 Features and Goals

The Cross-Agent Pattern Sharing system enables the collective memory and learning of the agent society. Rather than each agent operating in isolation per task, this component ensures that experiences, solutions, and reflections from one context are available to others later. Goals include:
	•	Global Vector Memory: Maintain a centralized (or distributed but shared) memory bank of all significant outputs, outcomes, and insights, indexed by semantic content. This is essentially a long-term memory implemented via a vector database (ChromaDB in our case) that any agent can query. Over time, this becomes a knowledge base of code snippets, solutions to bugs, design rationales, and more.
	•	Task-Type-Aware Indexing: Organize memory such that querying is efficient and relevant. Since coding, testing, refactoring, and documenting are different kinds of tasks with different data formats and semantics, the system should index and retrieve with awareness of task type. For example, when a coding agent searches memory, it should primarily get code or coding-related knowledge (not irrelevant documentation). This can be achieved by tagging entries with a type (code, test, doc, decision, etc.) and/or using separate collections per type.
	•	Reflection Mining: Capture not only final outputs but also reflections and retries. Often, the most valuable lessons are in the critiques and the adjustments made after failures. This feature involves mining the message log for reflective content: critiques (what went wrong), fixes (how it was solved), and even meta-reflections (an agent summarizing what it learned). By extracting and storing these, the system can prevent repeated mistakes and shorten future problem-solving (the agent can recall “Ah, we’ve seen a similar bug before and how we fixed it”).
	•	Cross-Agent Learning: Allow agents to learn from each other’s successes and failures. If Agent A solved a tricky problem, Agent B later should benefit. Concretely, if a test agent finds that a particular testing strategy uncovered a bug, all future test agents can adopt that strategy by retrieving that pattern from memory. This is analogous to how human developers share knowledge via documentation or code libraries.
	•	Preventing Redundancy and Stale Knowledge: As memory grows, there is risk of clutter (many similar entries, outdated info, etc.). The system must include processes to prune or consolidate knowledge. For example, if 5 tasks all ended up producing a nearly identical “util function” for parsing JSON, the memory might store 5 copies. We could periodically cluster similar embeddings and merge them, keeping one canonical solution (or abstracting them into a general form). Similarly, if a reflection is superseded by a later one (e.g. “We found a better way than our old workaround”), the older one might be marked deprecated. This maintenance ensures quality of the knowledge base.

3.2 Architecture

At the core is the Vector Database (ChromaDB) which stores entries representing knowledge items. Each item might originate from:
	•	Agent outputs: code files, test cases, documentation produced.
	•	Critic feedback: problems identified and resolved.
	•	Execution results: e.g. stack traces or performance metrics if captured.
	•	Summaries/reflections: an agent’s summary of how it solved something or why something failed.

Each entry is stored with:
	•	A high-dimensional embedding (using a model appropriate for content type – possibly multiple embedding models if needed, see below).
	•	Metadata tags: e.g. {"type": "code", "language":"Python", "purpose":"utility", "taskType":"data_processing"} or {"type":"reflection", "phase":"debugging", "issue":"null pointer", "resolved": true}.
	•	A reference to source (which task and message it came from, so one can retrieve full context if needed).
	•	The content or a link to content (for large artifacts like full code, we might store a short excerpt + a pointer to the code repository or file store).

Task-Type-Aware Indexing: We anticipate using separate collections or at least metadata filters for broad categories:
	•	Code Snippets or Patterns,
	•	Test Scenarios and Results,
	•	Documentation pieces (design docs, requirements),
	•	Reflections/Lessons.

This allows an agent to query within the relevant domain. For example, if an agent is writing code, it might do collection.query("embedding of my problem", filter={"type":"code"}) to find similar code. Or the system might automatically retrieve relevant documentation + code + known pitfalls separately and feed them as context (a multi-retrieval).

There’s relevant research in using RAG (Retrieval-Augmented Generation) for agents – essentially agents augment their prompts with retrieved relevant knowledge ￼ ￼. Our system does exactly that: before an agent tackles a task, it queries the knowledge base for related info (past tasks of similar nature, libraries, error resolutions, etc.). This is akin to how Voyager (Wang et al. 2023), an embodied lifelong learning agent, built an ever-growing skill library of code that it could reuse on new tasks ￼. Voyager’s skill memory allowed it to generalize to novel tasks by recalling previously learned skills instead of coding from scratch ￼. We aim for a similar outcome: over time, the agents accumulate a library of solutions and don’t need to reinvent the wheel for each new task.

Reflection Mining Process: We will likely implement a periodic background agent (or a special mode of the Critic agent) that goes through recent task transcripts to extract key reflections:
	•	It might identify all instances of “mistake made – correction – outcome” in the logs and summarize them as a lesson.
	•	For example, if in Task 45 the engineer struggled with an off-by-one error and fixed it after critique, the reflection miner could store an entry: type:"reflection", issue:"off-by-one loop", resolution:"use <= instead of <, and add unit test for boundary".
	•	These reflections can be short text statements which are highly semantically searchable. They might be stored with tags like {"issue":"off-by-one", "category":"bug", "language":"Python"} and an embedding of the text. Later, if an agent’s code has a loop, a smart prompt or static analyzer might search memory for “common loop errors” and retrieve this reflection, proactively warning the agent.

Quality of Embeddings: One challenge is embedding different data types. Code vs prose vs error logs each have different semantics. We may use:
	•	A general embedding model (like OpenAI’s text-embedding-ada) that works okay on code and text.
	•	Or specialized models for each type (a code-specific embedding model for code snippets, and a text model for comments/docs). ChromaDB can store all, possibly we maintain separate vectors or attach both code and text embeddings.

We must also consider the context window limitations: if we retrieve too many similar items, feeding them all to an LLM may be impossible. Instead, we might retrieve top N and perhaps let the agent choose which to apply. We could also incorporate some pattern abstraction: e.g. if many similar code solutions exist, store a single generalized solution to represent them.

3.3 Example Usage Scenarios

Scenario 1: Reusing Code Patterns – An agent is tasked with writing a function to parse CSV files. It queries the global memory: “find code for CSV parsing or similar ETL tasks.” The vector DB, using the query embedding, returns a snippet from a previous task that parsed TSV files (similar structure) and maybe another that dealt with parsing but in JSON (less similar). The agent finds the CSV code snippet highly relevant – it then adapts it to the new task, saving time and avoiding errors. It also retrieves any reflections associated: e.g. a note “previous parser had a bug with empty fields – ensure to handle that.” Armed with that knowledge, the agent writes a robust parser. This increases the cross-agent reuse rate: knowledge from a past task by a different agent (or earlier instance of same agent) is directly reused ￼.

Scenario 2: Avoiding Known Pitfalls – A testing agent is about to generate tests for a sorting algorithm. Before it starts, it searches memory for “sorting edge cases.” Suppose a reflection entry says: “Lesson from Task 30: when testing sorting, include cases with duplicate values and already-sorted input, as a bug was found when input was already sorted.” The agent incorporates this, generating tests for those scenarios. This kind of reflection mining ensures that once a bug has been discovered in one task, all future tasks can preemptively test for it. Over time, the number of unique new bugs should drop, as agents are not repeating mistakes – a measure of success for pattern sharing.

Scenario 3: Knowledge Consolidation – Over months, the agents have solved tasks involving database queries many times. We have multiple code snippets in memory about “SQL query building” and reflections like “always parameterize queries to prevent SQL injection.” A maintenance routine (which could be a developer or an automated process) identifies that knowledge and writes a Best Practice document. This document is stored in the memory (type: “guideline”, tags: {topic: “SQL”, security: true}). Now, when a future coding agent is about to write database code, the system can automatically fetch this best-practice and either feed it into the prompt or let the agent know such a guideline exists. Essentially, the collective learning has produced documentation that agents treat as authoritative. This reduces redundant entries and gives a single source of truth for certain patterns.

Scenario 4: Task-Type Segregated Recall – Consider an agent responsible for documentation (writing a README or API docs). It will query the memory differently: maybe looking for “past documentation for similar modules” (type: documentation). It could find how previous modules were documented and follow those patterns for consistency. Meanwhile, it likely ignores code embeddings. Conversely, a coding agent might ignore documentation entries unless it specifically needs context from them. By segmenting or filtering by type, each agent gets relevant info without noise. If needed, an agent can still cross-search other types by adjusting filters (e.g. a coder might search design docs if stuck on understanding a requirement).

Dealing with Redundancy: The system could implement a cron job or a special “Librarian Agent” whose job is to periodically curate the memory:
	•	It may merge duplicate entries: if 5 code solutions have 90% similarity, keep one representative (perhaps the latest or the one with best performance feedback).
	•	It may remove very low-quality entries: e.g. an output that turned out to be wrong and was later fixed – we don’t want the wrong one cluttering results. This requires tracking which outputs were superseded. We could mark in metadata deprecated: true for such entries.
	•	It might also compress knowledge: generate higher-level summaries. For instance, after 10 tasks on web scraping, produce an article “Common Web Scraping Techniques” that encapsulates those tasks’ knowledge. Storing that as a reference might be more useful than a bunch of raw logs.

Vector Update and Freshness: We will continuously add vectors for new messages and outputs. Ensuring embedding quality might involve re-embedding some content with updated models over time or normalizing embeddings. We should also monitor the vector recall accuracy – if agents start retrieving irrelevant stuff often, it suggests either the embedding model isn’t capturing the right similarity or we need better query strategies. One mitigation is to incorporate metadata filtering strongly (as said, by task type or tags) to narrow search scope, thereby improving precision of results ￼.

In practice, we expect an upward trend in efficiency as this global memory grows: agents spend less time on repetitive problems and fewer Critic interventions are needed for known issues (as agents proactively avoid them). This is analogous to a human team’s collective experience – over time, they make fewer mistakes on familiar tasks.

3.4 Tying it Together with Task-Kanban-MCP

In a Kanban-style workflow management (like task-kanban-mcp), each task moves from “To Do” to “In Progress” to “Done” columns. Our multi-agent setup plugs into this as follows:
	•	When a task is moved to “Done,” the outputs and lessons from it are immediately indexed into the global memory. Perhaps an automated step runs on task completion: gather the code, tests, final docs, and critique summary, and call the memory API to store them (with taskId and tags).
	•	When a new task is created or moves to “In Progress,” the assigned agent (or a helper agent) automatically performs a memory query for that task’s context. For example, based on the task description, it forms a query and fetches top 3 relevant items (maybe similar tasks from the past). Those could be displayed on the Kanban card as references, or directly given to the agent in its prompt to start.
	•	The Kanban system, being aware of task types (maybe via labels), can suggest which index to query. E.g., a task labeled “bug fix” might prompt a search in the reflections/bugs index for the error message or related component.

Real-World Inspiration: This approach is reminiscent of what some tools do with past knowledge. For instance, the paper “Buffer of Thoughts (BoT): Thought-Augmented Reasoning” (Yang et al. 2024) proposes a buffered memory of thought traces that can be reused to improve reasoning ￼ ￼. BoT acts as a structured memory of “thought templates” to solve new problems faster ￼. Our Cross-Agent Pattern Sharing is analogous – it’s a buffer of previous reasoning and solutions that agents draw upon, effectively making reasoning and coding not just chain-of-thought but graph-of-thought where past nodes are available. Another analogy is Ghost in the Minecraft (GITM) (2023) which gave agents long-term memory in a complex environment, improving their general capability by learning from past experiences ￼ ￼.

By implementing Cross-Agent Pattern Sharing, we aim for continuous improvement of the multi-agent system: each task makes the system a little smarter. The vector database ensures those improvements are retained and efficiently accessible.

4. Performance Metrics and Evaluation

To validate the effectiveness of these systems (Critic agent, Messaging, Pattern Sharing), we will track several metrics:
	•	Critique Success Rate: How often does the Critic agent’s feedback result in a successful improvement? Concretely, this can be measured as the percentage of Critic-flagged issues that are resolved in subsequent iterations. For example, if the Critic finds 5 bugs and after the refinement loop 5/5 are fixed, that’s 100%. Another measure is the reduction in failed test cases or errors after critique. In research like Reflexion, iterative self-critique raised success on coding tasks dramatically (e.g. HumanEval pass rate from 80% to 91% with GPT-4) ￼ – we can track a similar before/after performance for tasks with Critic vs without. A high critique success rate indicates the Critic’s feedback is useful and correctly applied by agents.
	•	Turnaround Time / Efficiency: Because we introduced asynchronous flows, we should ensure overall task completion time improves (or at least doesn’t regress) compared to a synchronous approach. We can measure average task duration and idle times. If our message queue batching is effective, agents should spend less total waiting time. We also monitor token usage (since multi-agent messages can blow up context). Metrics like “tokens consumed per task” or cost per task can be tracked. Communication overhead is something to minimize – e.g. MegaAgent notes that as agents/rounds increase, token cost grows, so they aim to reduce unnecessary comms ￼ ￼. We similarly want to see if using a Critic with succinct feedback and pattern memory reduces the total number of back-and-forth messages required to reach completion.
	•	Cross-Agent Reuse Rate: What fraction of new outputs contain or reference elements from the global memory? For instance, if a new code task is solved using a snippet from memory, mark that as a reuse. We can instrument agents to log whenever they fetch something from memory that they actually incorporate (maybe via comparing similarity of new code to retrieved code to detect reuse). A rising reuse rate over time would indicate pattern sharing is working (agents increasingly leverage prior work rather than starting fresh). Ideally, trivial tasks eventually approach 100% reuse (just stitching known components), whereas novel tasks might be lower but should climb as more knowledge accumulates.
	•	Error Reduction Over Time: Track the frequency of common errors/bugs over tasks. If pattern sharing is effective, we expect a learning curve – e.g., the first few times a type of bug appears, Critic catches it, and a reflection is stored. Later, that bug should appear less frequently as agents preempt it. We can maintain a catalog of error types (null pointer, off-by-one, etc.) and measure occurrences per 100 tasks over time – should trend downward for known types. Similarly, measure test failure rates: ideally, as agents learn, the proportion of tasks passing all tests on first try should increase.
	•	Consensus Correctness: For tasks requiring multi-agent agreement (like design decisions or debates), measure the quality of the final decision. This can be done via evaluation against a ground truth if available, or human expert judgment on the outcome. If using multi-agent debate (Critic orchestrated or otherwise), we’d compare correctness of the consensus answer vs a single-agent answer. Research suggests multi-agent debate can improve truthfulness and reasoning accuracy ￼; we’d verify that in our domain (for example, does having two agents debate a tricky algorithm yield a more correct solution than one agent alone?). A possible metric: tasks solved correctly with debate vs without, or an improvement in solution quality score.
	•	Communication Load Metrics: Count messages per task, and average size. Are agents spamming each other or is communication efficient? The sparse-topology idea suggests fewer connections can achieve same results ￼. We might experiment with limiting certain communications and see if performance holds – if yes, that’s good for efficiency. We can define an ideal range of messages for a given task complexity and measure against it.
	•	Knowledge Base Quality: Although harder to quantify directly, we can evaluate the knowledge base via information retrieval metrics. For instance, periodically sample a query relevant to a known item and see if the correct item is in the top-K results (precision@K). Or track how often agents find something useful vs come up empty (a query that returns nothing useful might indicate a gap to fill). Also, monitor memory size and redundancy – e.g. ratio of distinct solutions to total solutions stored. A high redundancy ratio might prompt a curation action.
	•	Human Oversight Metrics: If any human is in the loop (maybe reviewing final outputs occasionally), measure how often they accept outputs without changes. The goal is to minimize human intervention while maintaining quality. A decrease in required human corrections over time would show the system is learning and improving reliability.

Finally, qualitative evaluation is also important:
	•	Conduct case studies on certain complex tasks and document how the multi-agent system handled them, highlighting if the Critic found subtle bugs or if memory lookup saved the day.
	•	Solicit developer feedback (if developers use the system’s outputs): are the code and tests improving in readability and correctness?

For continuous improvement, we can adopt a feedback loop on these metrics. For instance, if critique success rate is low in some area, we analyze why – maybe the Critic’s prompt needs tuning or agents aren’t applying feedback correctly. If reuse rate is low, perhaps the retrieval isn’t bringing relevant info (embedding issues) – we might improve embeddings or add more metadata.

In summary, success will be indicated by faster task completion with fewer errors as the system scales, thanks to the Critic catching issues early and the shared memory preventing repetition. We expect to see fewer loops needed per task (e.g., tasks getting done in 1-2 iterations rather than 5-6 after the system has learned), and higher overall solution quality (as measured by tests and external evaluation). Through these metrics, we will rigorously validate that asynchronous multi-agent collaboration with LLMs – with a Critic in the loop, a solid comms framework, and collective learning – leads to more efficient and reliable software development workflows.

References: The design above incorporates insights from recent research and implementations, including multi-agent collaboration frameworks like MetaGPT ￼ ￼, Autogen ￼, and ChatDev; self-refinement and critique techniques ￼ ￼; multi-agent debate for improved reasoning ￼; lifelong learning agents like Voyager ￼; and memory-augmented reasoning approaches like Buffer-of-Thoughts ￼, among others. These sources have demonstrated the potential of cooperative LLM agents to outperform single agents in both quality and adaptability, guiding the architecture and best practices detailed in this PRD.